# -*- coding: utf-8 -*-
"""AI ML Engineer Tech Take-Home Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bKVceizCLe-F7e9fPuNniHtZ1T5zWrO3
"""

import pandas as pd

import csv

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files

files.upload()

df=pd.read_csv('data.csv')
df.head(100)

"""**To show the info about all the feautres**"""

# prompt: code to shows all the feautures

df.info()

"""**size of the data**"""

# prompt: size of the data

df.shape

"""**Part 1-Data Visulaization**

Distribution of Sentiment Across Airlines: Compare the distribution of
sentiments (positive, neutral,
negative) across different airlines.
"""

!pip install google.colab

"""**Distribution of Sentiment Across Airlines**: Compare the distribution of sentiments (positive, neutral,
negative) across different airlines.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import Image
from google.colab import files

# Assuming your DataFrame is named 'df'
sns.countplot(x='airline', hue='airline_sentiment', data=df)
plt.title('Distribution of Sentiment Across Airlines')


# Save the plot as an image file
plt.savefig('sentiment_distribution.png')

# Download the saved image
files.download('sentiment_distribution.png')

# prompt: to check the name under 	airline feauture

df.airline.unique()

"""**Analysis of Negative Tweet Reasons by Airline**: Identify the most common reasons for negative
sentiments towards each airline.
"""

# prompt: Analysis of Negative Tweet Reasons by Airline: Identify the most common reasons for negative sentiments towards each airline.

df[df['airline_sentiment']=='negative'].groupby(['airline']).value_counts().head(5)

plt.figure(figsize=(12, 6))
sns.countplot(x='negativereason', hue='airline', data=df[df['airline_sentiment']=='negative'])
plt.title('Common Negative Tweet Reasons by Airline')
plt.xticks(rotation=45, ha='right')
# Save the plot as an image file
plt.savefig('Analysis of Negative Tweet Reasons by Airline.png')

# Download the saved image
files.download('Analysis of Negative Tweet Reasons by Airline.png')

"""**Relationship Between Sentiment Confidence and Retweet Count**: Examine if tweets with higher
sentiment confidence are more likely to be retweeted.
"""

# prompt: Relationship Between Sentiment Confidence and Retweet Count: Examine if tweets with higher sentiment confidence are more likely to be retweeted.

plt.figure(figsize=(12, 6))
sns.scatterplot(x='airline_sentiment_confidence', y='retweet_count', data=df)
plt.title('Relationship Between Sentiment Confidence and Retweet Count')
plt.xlabel('Sentiment Confidence')
plt.ylabel('Retweet Count')

# Save the plot as an image file
plt.savefig('Relationship Between Sentiment Confidence and Retweet Count.png')

# Download the saved image
files.download('Relationship Between Sentiment Confidence and Retweet Count.png')

df.retweet_count.unique()

df.airline_sentiment_confidence.unique()

"""**Part 2 - Sentiment Analysis Model**

1. Implement a machine learning model to predict the sentiment ( airline_sentiment ) of a tweet based on
its contents.

Importing the libraries
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""Loading and preprocessing of the data

To remove HTTPS
"""

import re

def remove_urls(text):
    # Regular expression to match URLs
    url_pattern = r"https?://\S+|www\.\S+"
    return re.sub(url_pattern, '', text)

# Apply the custom function to the 'clean_tweet' column
df['clean_tweet'] = df['text'].apply(remove_urls)

df.head(5000)

import pandas as pd

# Set the display option to show the full content of 'clean_tweet' column
pd.set_option('display.max_colwidth', None)

# Print the 'clean_tweet' column
print(df['clean_tweet'])

"""to download the data for a check"""

from google.colab import files
df.to_csv('httpremoved.csv')
files.download('httpremoved.csv')

"""Stop words Removal"""

import nltk

nltk.download('stopwords')

nltk.download('punkt')

import nltk
from nltk.corpus import stopwords


# Read the CSV file into a DataFrame
df = pd.read_csv('httpremoved.csv')  # Replace 'your_file.csv' with the actual file path

# Function to clean text data
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Tokenize the text into words
    words = nltk.word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Join the words back into a clean text
    clean_text = ' '.join(words)

    return clean_text

# Apply the clean_text function to the 'POST_TEXT' column and create a new column 'CLEANED_TEXT'
df['clean_tweet'] = df['clean_tweet'].apply(clean_text)

df.head(5000)

"""to extract#"""

def extract_hashtags(text):
    return [word for word in text.split() if word.startswith('#')]

# Apply the custom function to the 'POST_TEXT' column
df['Hashtags'] = df['clean_tweet'].apply(extract_hashtags)

# Print the updated DataFrame
print(df)

# Define a custom function to remove hashtags from text
def remove_hashtags(text):
    # Split the text into words and filter out words starting with '#'
    return ' '.join([word for word in text.split() if not word.startswith('#')])

# Apply the custom function to the 'POST_TEXT' column
df['clean_tweet'] = df['clean_tweet'].apply(remove_hashtags)

# Print the updated DataFrame
print(df)

df[['text','clean_tweet']]

"""removing multiple white spaces"""

# Define a custom function to remove multiple white spaces
def remove_multiple_spaces(text):
    return ' '.join(text.split())

# Apply the custom function to the 'clean_tweet' column
df['clean_tweet'] = df['clean_tweet'].apply(remove_multiple_spaces)

"""removing urls"""

import re
import pandas as pd



# Specify the column containing the text data
column_name = 'clean_tweet'  # Replace 'your_column_name' with the name of the column containing the text data

# Function to remove URLs from text
def remove_urls(text):
    return re.sub(r'https?://\S+|www\.\S+', '', text)

# Apply the remove_urls function to the specified column
df['clean_tweet'] = df['clean_tweet'].apply(remove_urls)

# Print the cleaned DataFrame
print(df)

import string

"""Defining a custom function to remove punctuation from text"""

# Define a custom function to remove punctuation from text
def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

# Apply the custom function to the 'clean_tweet' column
df['clean_tweet'] = df['clean_tweet'].apply(remove_punctuation)

"""**to remove sepcial characters and emojis**"""

import pandas as pd
import re

# Define the custom function to remove special characters and emojis
def remove_special_characters_and_emojis(text):
    # Remove special characters using regex
    text = re.sub(r'[^\w\s]', '', text)

    # Remove emojis using regex
    text = re.sub(r'[^\x00-\x7F]+', '', text)

    return text

# Assuming you have a DataFrame 'df' with a 'clean_tweet' column
# Apply the custom function to the 'clean_tweet' column and save the result back to the same column
df['clean_tweet'] = df['clean_tweet'].apply(remove_special_characters_and_emojis)

# Display the DataFrame with cleaned text
print(df)

# Assuming 'df' is your DataFrame
df.to_csv('output_data.csv', index=False)

"""Lemmatization is the process of finding the form of the related word in the dictionary while Stemming is the process of finding the root of words and Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. I will be carrying out the model prediction on the lemmatized text, because the text will loose the original meaning compared to Tokenization and stemming"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# Download NLTK resources if not already downloaded
nltk.download('punkt')
nltk.download('wordnet')

# Load the CSV file into a DataFrame
df = pd.read_csv('output_data.csv')

# Check for and handle missing values
df['clean_tweet'].fillna('', inplace=True)

# Define functions for tokenization, lemmatization, and stemming
def tokenize(text):
    return word_tokenize(text)

def lemmatize(tokens):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(token) for token in tokens]

def stem(tokens):
    stemmer = PorterStemmer()
    return [stemmer.stem(token) for token in tokens]

# Apply tokenization, lemmatization, and stemming to the 'clean_tweet' column
df['tokenized_tweet'] = df['clean_tweet'].apply(tokenize)
df['lemmatized_tweet'] = df['tokenized_tweet'].apply(lemmatize)
df['stemmed_tweet'] = df['tokenized_tweet'].apply(stem)

# Print the resulting DataFrame with tokenized, lemmatized, and stemmed columns
print(df)

df

"""Join tokens back into single strings"""

# Join tokens back into single strings
df['tokenized_tweet'] = df['tokenized_tweet'].apply(lambda tokens: ' '.join(tokens))
df['lemmatized_tweet'] = df['lemmatized_tweet'].apply(lambda tokens: ' '.join(tokens))
df['stemmed_tweet'] = df['stemmed_tweet'].apply(lambda tokens: ' '.join(tokens))

# Print the resulting DataFrame with tokenized, lemmatized, and stemmed columns
print(df)

df

"""The dataset is split into 80 to 20 ratio, 80% is used for training the machine learning models while 20% was used for testing the models."""

from sklearn.model_selection import train_test_split

train,valid = train_test_split(df,test_size = 0.2,random_state=0,stratify = df.airline_sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.
print("train shape : ", train.shape)
print("valid shape : ", valid.shape)

"""**Feature extraction**:
In this work, the **count Vectorizer** technique was adopted for feature extraction. This technique has been widely adopted for solving classification problems. CountVectorizer is a simple vectorization method, the library in Python is given by Scikit-learn. It converts a set of tweets into a vector space with a matrix of token counts. It covers tweets based on the most frequently occurring terms (count) in the tweets. The count vectorizer generates a word matrix where each distinct word serves as the matrix's column and the selected text from the documents serves as the matrix's row.  This enables the word in the text sample to be counted

**Starting with the simple model**
"""

# Remove rows with NaN values from the training and validation dataframes
train = train.dropna(subset=['lemmatized_tweet'])
valid = valid.dropna(subset=['lemmatized_tweet'])

# Reindex the dataframes after removing rows
train = train.reset_index(drop=True)
valid = valid.reset_index(drop=True)

# Now, you can proceed with the vectorization
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

stop = list(stopwords.words('english'))
vectorizer = CountVectorizer(decode_error='replace', stop_words=stop)

X_train = vectorizer.fit_transform(train.clean_tweet.values)
X_valid = vectorizer.transform(valid.clean_tweet.values)

y_train = train.airline_sentiment.values
y_valid = valid.airline_sentiment.values

print("X_train.shape : ", X_train.shape)
print("X_valid.shape : ", X_valid.shape)
print("y_train.shape : ", y_train.shape)
print("y_valid.shape : ", y_valid.shape)

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
stop = list(stopwords.words('english'))
vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)

X_train = vectorizer.fit_transform(train.clean_tweet.values)
X_valid = vectorizer.transform(valid.clean_tweet.values)

y_train = train.airline_sentiment.values
y_valid = valid.airline_sentiment.values

print("X_train.shape : ", X_train.shape)
print("X_train.shape : ", X_valid.shape)
print("y_train.shape : ", y_train.shape)
print("y_valid.shape : ", y_valid.shape)

# example of grid searching key hyperparametres for logistic regression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression


# define models and parameters
model = LogisticRegression()
logReg = LogisticRegression(solver='lbfgs', max_iter=4000)  # New model
solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01, 0.001]

# define grid search
grid = dict(solver=solvers, penalty=penalty, C=c_values)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=1, cv=cv,
                           scoring='accuracy',error_score=0)
grid_result= grid_search.fit(X_train, y_train)

# summarize results
print(f"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}")

"""After applying the grid search for the hyperparameter tunning optimization, the best hyperparameters for Logistic regression analysis are 0.788 best grid result
 from Best: 0.788 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}
"""

from sklearn.model_selection import RepeatedStratifiedKFold

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB

naiveByes_clf = MultinomialNB()

naiveByes_clf.fit(X_train,y_train)

NB_prediction = naiveByes_clf.predict(X_valid)
NB_accuracy = accuracy_score(y_valid,NB_prediction)
print("training accuracy Score    : ",naiveByes_clf.score(X_train,y_train))
print("Validation accuracy Score : ",NB_accuracy )
print(classification_report(NB_prediction,y_valid))

from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)

sgd_clf.fit(X_train,y_train)

sgd_prediction = sgd_clf.predict(X_valid)
sgd_accuracy = accuracy_score(y_valid,sgd_prediction)
print("Training accuracy Score    : ",sgd_clf.score(X_train,y_train))
print("Validation accuracy Score : ",sgd_accuracy )
print(classification_report(sgd_prediction,y_valid))

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=6)

rf_clf.fit(X_train,y_train)

rf_prediction = rf_clf.predict(X_valid)
rf_accuracy = accuracy_score(y_valid,rf_prediction)
print("Training accuracy Score    : ",rf_clf.score(X_train,y_train))
print("Validation accuracy Score : ",rf_accuracy )
print(classification_report(rf_prediction,y_valid))

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# Assuming y_train and y_valid are pandas Series containing string labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_valid_encoded = label_encoder.transform(y_valid)

# Initialize and train XGBoost Classifier
xgboost_clf = xgb.XGBClassifier()
xgboost_clf.fit(X_train, y_train_encoded)

# Make predictions on the validation set
xgb_prediction = xgboost_clf.predict(X_valid)
xgb_accuracy = accuracy_score(y_valid_encoded, xgb_prediction)

# Print accuracy scores
print("Training accuracy Score    : ", xgboost_clf.score(X_train, y_train_encoded))
print("Validation accuracy Score : ", xgb_accuracy)
print(classification_report(xgb_prediction, y_valid_encoded))

from sklearn.svm import SVC

svc = SVC()

svc.fit(X_train, y_train)

svc_prediction = svc.predict(X_valid)
svc_accuracy = accuracy_score(y_valid,svc_prediction)
print("Training accuracy Score    : ",svc.score(X_train,y_train))
print("Validation accuracy Score : ",svc_accuracy )
print(classification_report(svc_prediction,y_valid))

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
logreg_prediction = logreg.predict(X_valid)
logreg_accuracy = accuracy_score(y_valid,logreg_prediction)
print("Training accuracy Score    : ",logreg.score(X_train,y_train))
print("Validation accuracy Score : ",logreg_accuracy )
print(classification_report(logreg_prediction,y_valid))

models = pd.DataFrame({
    'Model': ['Support Vector Machines', 'Logistic Regression',
              'Random Forest', 'Naive Bayes',
              'Stochastic Gradient Decent', 'XGBoost'],
    'Training accuracy': [svc_accuracy, logreg_accuracy,
              rf_accuracy, NB_accuracy,
              sgd_accuracy, xgb_accuracy,]})

models.sort_values(by='Training accuracy', ascending=False)

"""**Logistic regression** performs best. Therefore, am going to progressively
 refine the model by experimenting with various data
transformation techniques to improve its accuracy.

**TF-IDF Transformation:** TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). TF-IDF is commonly used in natural language processing and information retrieval for text analysis and feature extraction.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming X_train and X_valid are your text data

# Convert data to strings if not already
X_train = [str(text) for text in X_train]
X_valid = [str(text) for text in X_valid]

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(lowercase=False)  # Disable lowercase if it's already done in a pre-processing step

# Fit and transform on training data and transform validation data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_valid_tfidf = tfidf_vectorizer.transform(X_valid)

# Train Logistic Regression on TF-IDF transformed data
logreg_tfidf = LogisticRegression()
logreg_tfidf.fit(X_train_tfidf, y_train)

# Make predictions on the validation set
logreg_prediction_tfidf = logreg_tfidf.predict(X_valid_tfidf)
logreg_accuracy_tfidf = accuracy_score(y_valid, logreg_prediction_tfidf)

# Print accuracy scores for TF-IDF transformed data
print("Training accuracy Score (TF-IDF): ", logreg_tfidf.score(X_train_tfidf, y_train))
print("Validation accuracy Score (TF-IDF): ", logreg_accuracy_tfidf)
print(classification_report(logreg_prediction_tfidf, y_valid))

"""**N-grams:**
N-grams are contiguous sequences of n items (or words) from a given sample of text or speech. In the context of natural language processing, these items are often words, but they can also be characters or other units of language. N-grams are used to capture the local structure and context of a sequence of words.
"""

# Experiment with using bi-grams and tri-grams in addition to unigrams
ngram_vectorizer = TfidfVectorizer(ngram_range=(1, 3))

# Fit and transform on training data and transform validation data
X_train_ngrams = ngram_vectorizer.fit_transform(X_train)
X_valid_ngrams = ngram_vectorizer.transform(X_valid)

# Train Logistic Regression on N-grams transformed data
logreg_ngrams = LogisticRegression()
logreg_ngrams.fit(X_train_ngrams, y_train)

# Make predictions on the validation set
logreg_prediction_ngrams = logreg_ngrams.predict(X_valid_ngrams)
logreg_accuracy_ngrams = accuracy_score(y_valid, logreg_prediction_ngrams)

# Print accuracy scores for N-grams transformed data
print("Training accuracy Score (N-grams): ", logreg_ngrams.score(X_train_ngrams, y_train))
print("Validation accuracy Score (N-grams): ", logreg_accuracy_ngrams)
print(classification_report(logreg_prediction_ngrams, y_valid))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming X_train and X_valid are your text data
# Assuming y_train and y_valid are your target labels

# Convert data to strings if not already
X_train = [str(text) for text in X_train]
X_valid = [str(text) for text in X_valid]

# Initialize a DataFrame to store the results
results_df = pd.DataFrame(columns=['Transformation', 'Training Accuracy', 'Validation Accuracy'])

# TF-IDF Transformation
tfidf_vectorizer = TfidfVectorizer(lowercase=False)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_valid_tfidf = tfidf_vectorizer.transform(X_valid)

logreg_tfidf = LogisticRegression()
logreg_tfidf.fit(X_train_tfidf, y_train)

logreg_prediction_tfidf = logreg_tfidf.predict(X_valid_tfidf)
logreg_accuracy_tfidf = accuracy_score(y_valid, logreg_prediction_tfidf)

# Append results to the DataFrame
results_df = results_df.append({'Transformation': 'TF-IDF',
                                'Training Accuracy': logreg_tfidf.score(X_train_tfidf, y_train),
                                'Validation Accuracy': logreg_accuracy_tfidf},
                               ignore_index=True)

# N-grams Transformation
ngram_vectorizer = TfidfVectorizer(ngram_range=(1, 3))
X_train_ngrams = ngram_vectorizer.fit_transform(X_train)
X_valid_ngrams = ngram_vectorizer.transform(X_valid)

logreg_ngrams = LogisticRegression()
logreg_ngrams.fit(X_train_ngrams, y_train)

logreg_prediction_ngrams = logreg_ngrams.predict(X_valid_ngrams)
logreg_accuracy_ngrams = accuracy_score(y_valid, logreg_prediction_ngrams)

# Append results to the DataFrame
results_df = results_df.append({'Transformation': 'N-grams',
                                'Training Accuracy': logreg_ngrams.score(X_train_ngrams, y_train),
                                'Validation Accuracy': logreg_accuracy_ngrams},
                               ignore_index=True)

# Print the DataFrame
print(results_df)

"""It can be seen form the data trasformation techniques that **TF-IDF**  used to tunned the model performed best with data

selecting the best performed Nodel and Progressively refine the model by experimenting with various data
transformation techniques to improve accuracy
makes **TF-IDF** to perform best in tunning the Logistic regression model with acuracy value of  **0.785519** more than  N-grams accuracy of **0.771858** and also more than initial Logistic regression accuracy of **0.782104**
"""

